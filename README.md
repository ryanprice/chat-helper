# chat-helper

A self-hosted Signal bot that runs as a linked device on your account and brings agentic AI tools into your chats. Send a slash command â€” with a replied-to message, an inline URL, or plain text â€” and the bot researches, expands, or condenses the content using a local LLM.

<img width="581" height="442" alt="image" src="https://github.com/user-attachments/assets/3c28cbab-ec74-4834-a7a4-79542164dec5" />

---

## Features

- **`/e [1â€“10]`** â€” Expand and research content. Level 1 is a single sentence; level 10 is an exhaustive deep-dive. Default: 5.
- **`/c [1â€“10]`** â€” Condense content. Level 1 is a light trim; level 10 is one to five words. Default: 5.
- **`/h`** â€” Print help text in the chat.
- **Flexible input** â€” commands work as a reply to a quoted message, or with a URL/text included inline:
  ```
  /e 3
  /c https://example.com/article
  https://www.youtube.com/watch?v=... /e 7
  /c 6 https://example.com
  ```
- **Smart content routing** â€” YouTube URLs â†’ transcript fetch; other URLs â†’ page fetch; no URL â†’ web search.
- **Instant acknowledgment** â€” a `ã€”ğŸ¤–ğŸ¤”...ã€•` message is sent immediately so chat participants know the command was received.
- **Reply routing** â€” the bot owner gets responses in-channel; all other allowed users get a DM.
- **Allowlist** â€” optionally restrict the bot to a specific list of phone numbers.

---

## How it works

chat-helper runs as a [linked device](https://support.signal.org/hc/en-us/articles/360007320551) on your Signal account, similar to Signal Desktop. It connects to [signal-cli-rest-api](https://github.com/bbernhard/signal-cli-rest-api) via WebSocket to receive messages, and uses a local [Ollama](https://ollama.com) instance as its LLM backend.

```
Signal WebSocket
      â”‚
      â–¼
 parse_envelope()
      â”‚
      â–¼
 _parse_command()   â† finds command anywhere in message, extracts level + inline text
      â”‚
      â”œâ”€ /h â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º send help to chat
      â”‚
      â”œâ”€ /e or /c
      â”‚       â”‚
      â”‚       â–¼
      â”‚   resolve content
      â”‚   (quote text â†’ inline text/URL â†’ error)
      â”‚       â”‚
      â”‚       â–¼
      â”‚   ğŸ¤–ğŸ¤” acknowledgment sent immediately
      â”‚       â”‚
      â”‚       â–¼
      â”‚   tool loop (LLM + tools, up to MAX_TOOL_ITERATIONS)
      â”‚       â”œâ”€ YouTube URL   â†’ get_transcript
      â”‚       â”œâ”€ other URL     â†’ fetch_page
      â”‚       â””â”€ no URL        â†’ web_search
      â”‚       â”‚
      â”‚       â–¼
      â”‚   _wrap() â†’ framed response
      â”‚       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â–¼
          owner? â†’ reply in-channel
          other? â†’ DM
```

---

## Requirements

- Docker + Docker Compose
- [Ollama](https://ollama.com) running locally
- A [Brave Search API key](https://brave.com/search/api/) (free tier: 1 req/sec)
- A Signal account to link the bot to

---

## Setup

### 1. Clone the repo

```bash
git clone <repo-url>
cd chat-helper
```

### 2. Configure environment

```bash
cp .env.example .env
```

Edit `.env`:

```env
SIGNAL_PHONE_NUMBER=+1234567890      # E.164 format â€” your Signal account number
BRAVE_API_KEY=your_key_here
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=glm-4.7-flash
ALLOWED_NUMBERS=                     # comma-separated E.164, or leave empty for all
TOOL_USE_FALLBACK=true               # set true if your model doesn't emit native tool_calls
MAX_TOOL_ITERATIONS=5
```

### 3. Pull the model

```bash
ollama pull glm-4.7-flash
```

Any Ollama model that supports tool use will work. `TOOL_USE_FALLBACK=true` enables a regex fallback for models that output tool calls as text rather than structured JSON.

### 4. Start signal-cli-rest-api and link your account

```bash
docker compose up signal-cli-rest-api
```

Open `http://localhost:8080/v1/qrcodelink?device_name=chat-helper` in a browser and scan the QR code with your Signal app (just like adding Signal Desktop).

### 5. Start the full stack

```bash
docker compose up
```

---

## Running locally (without Docker)

You'll still need signal-cli-rest-api running in Docker for the Signal WebSocket.

```bash
pip install -r requirements.txt
python -m src.main
```

> **Linux + Ollama note:** Ollama binds to `127.0.0.1` by default, so the Docker container can't reach it. Fix:
> ```bash
> sudo systemctl edit ollama
> # add under [Service]:
> # Environment="OLLAMA_HOST=0.0.0.0"
> sudo systemctl restart ollama
> ```

---

## Usage examples

| Message | What happens |
|---|---|
| Reply to a message + `/e` | Expands the quoted message at level 5 |
| Reply to a message + `/c 8` | Condenses the quoted message to near-minimum |
| `/e https://example.com/article` | Fetches the page and expands it |
| `/c 6 https://example.com/article` | Fetches the page and condenses at level 6 |
| `https://www.youtube.com/watch?v=xxx /e` | Fetches the YouTube transcript and expands it |
| `/e some topic I want to know about` | Web-searches the topic and expands the results |
| `/h` | Prints help in the current chat |

Responses are visually framed so they stand out in Signal:

```
ã€”ğŸ¤– chat-helperã€•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
...response text...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## Project structure

```
src/
â”œâ”€â”€ main.py            # entry point â€” WebSocket listener
â”œâ”€â”€ agent.py           # command parsing, tool loop, reply routing
â”œâ”€â”€ signal_client.py   # Signal API send/receive wrappers
â”œâ”€â”€ ollama_client.py   # Ollama chat API wrapper
â”œâ”€â”€ models.py          # InboundMessage, Quote, GroupInfo dataclasses
â”œâ”€â”€ config.py          # Settings loaded from .env
â”œâ”€â”€ conversation.py    # placeholder (stateless â€” no history stored)
â””â”€â”€ tools/
    â”œâ”€â”€ registry.py    # TOOL_REGISTRY + TOOL_DEFINITIONS for the LLM
    â”œâ”€â”€ web_search.py  # Brave Search API
    â”œâ”€â”€ transcript.py  # YouTube transcript fetcher
    â””â”€â”€ fetch_page.py  # Generic web page content fetcher
```

---

## Adding a new command

1. Add the command string to `COMMANDS` in `src/agent.py`.
2. Add a handler method (`_run_yourcommand`) following the pattern of `_run_expand` / `_run_condense`.
3. Wrap the reply with `_wrap()` before sending.

## Adding a new tool

1. Create an `async def your_tool(...)` function in `src/tools/your_tool.py`.
2. Register it in `TOOL_REGISTRY` in `src/tools/registry.py`.
3. Add its JSON schema to `TOOL_DEFINITIONS` with a clear description telling the LLM when to use it.

---

## Security

- Logs never contain message content â€” only metadata (envelope fields, phone numbers, message lengths).
- Brave Search `max_results` is hard-capped at 10 regardless of what the LLM requests.
- Unauthorized numbers are silently dropped â€” no response is sent, to avoid confirming the bot exists.
- `fetch_page` sends a generic `User-Agent` and forwards no cookies or credentials.
- Inline content is wrapped in `<quote>` tags with a system prompt instruction to treat it as data only, not instructions (prompt injection hardening).

---

## Dependencies

| Package | Purpose |
|---|---|
| `httpx` | HTTP client (Brave Search, page fetch) |
| `websockets` | Signal WebSocket listener |
| `python-dotenv` | `.env` loading |
| `youtube-transcript-api` | YouTube caption fetching |
| `beautifulsoup4` | HTML stripping for `fetch_page` |

---

## License

MIT
